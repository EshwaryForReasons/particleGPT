W0708 17:03:37.860000 139753309062976 torch/distributed/run.py:779] 
W0708 17:03:37.860000 139753309062976 torch/distributed/run.py:779] *****************************************
W0708 17:03:37.860000 139753309062976 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0708 17:03:37.860000 139753309062976 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_9_5
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_9_5
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_9_5
/pscratch/sd/e/eshy/particleGPT/train_new.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train_new.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train_new.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_9_5
/pscratch/sd/e/eshy/particleGPT/train_new.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Training progress: saving current checkpoint @ val_loss 4.483893871307373
AllOutput: Training progress: saving current checkpoint @ val_loss 4.480259895324707
AllOutput: Training progress: saving current checkpoint @ val_loss 4.47949743270874
AllOutput: Training progress: saving best checkpoint @ val_loss 4.479016304016113
AllOutput: Training progress: saving current checkpoint @ val_loss 4.479016304016113
AllOutput: Training progress: saving current checkpoint @ val_loss 4.479456901550293
AllOutput: Training progress: saving best checkpoint @ val_loss 4.477328300476074
AllOutput: Training progress: saving current checkpoint @ val_loss 4.477328300476074
AllOutput: Training progress: saving best checkpoint @ val_loss 4.47717809677124
AllOutput: Training progress: saving current checkpoint @ val_loss 4.47717809677124
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4770660400390625
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4770660400390625
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4778971672058105
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4775590896606445
AllOutput: Training progress: saving current checkpoint @ val_loss 4.477661609649658
AllOutput: Training progress: saving best checkpoint @ val_loss 4.476889133453369
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476889133453369
AllOutput: Training progress: saving best checkpoint @ val_loss 4.476583480834961
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476583480834961
AllOutput: Training progress: saving best checkpoint @ val_loss 4.476171493530273
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476171493530273
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4761762619018555
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476597785949707
AllOutput: Training progress: saving best checkpoint @ val_loss 4.475975513458252
AllOutput: Training progress: saving current checkpoint @ val_loss 4.475975513458252
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476211071014404
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4752302169799805
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4752302169799805
AllOutput: Training progress: saving best checkpoint @ val_loss 4.474875450134277
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474875450134277
AllOutput: Training progress: saving best checkpoint @ val_loss 4.474608898162842
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474608898162842
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4746012687683105
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4746012687683105
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473709583282471
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473709583282471
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473260402679443
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473260402679443
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473467826843262
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473188877105713
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473188877105713
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473159313201904
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473159313201904
AllOutput: Training progress: saving best checkpoint @ val_loss 4.472854137420654
AllOutput: Training progress: saving current checkpoint @ val_loss 4.472854137420654
slurmstepd: error: *** STEP 40209413.0 ON nid008605 CANCELLED AT 2025-07-11T00:03:35 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40209413 ON nid008605 CANCELLED AT 2025-07-11T00:03:35 DUE TO TIME LIMIT ***
