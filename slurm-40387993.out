W0710 06:09:42.333000 140606184417088 torch/distributed/run.py:779] 
W0710 06:09:42.333000 140606184417088 torch/distributed/run.py:779] *****************************************
W0710 06:09:42.333000 140606184417088 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0710 06:09:42.333000 140606184417088 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Training progress: saving current checkpoint @ val_loss 10.994131088256836
AllOutput: Training progress: saving best checkpoint @ val_loss 6.5947113037109375
AllOutput: Training progress: saving current checkpoint @ val_loss 6.5947113037109375
AllOutput: Training progress: saving best checkpoint @ val_loss 6.354158878326416
AllOutput: Training progress: saving current checkpoint @ val_loss 6.354158878326416
AllOutput: Training progress: saving best checkpoint @ val_loss 6.287874698638916
AllOutput: Training progress: saving current checkpoint @ val_loss 6.287874698638916
AllOutput: Training progress: saving best checkpoint @ val_loss 6.237492561340332
AllOutput: Training progress: saving current checkpoint @ val_loss 6.237492561340332
AllOutput: Training progress: saving best checkpoint @ val_loss 6.216001033782959
AllOutput: Training progress: saving current checkpoint @ val_loss 6.216001033782959
AllOutput: Training progress: saving best checkpoint @ val_loss 6.194222450256348
AllOutput: Training progress: saving current checkpoint @ val_loss 6.194222450256348
AllOutput: Training progress: saving best checkpoint @ val_loss 6.157246112823486
AllOutput: Training progress: saving current checkpoint @ val_loss 6.157246112823486
AllOutput: Training progress: saving best checkpoint @ val_loss 6.144389629364014
AllOutput: Training progress: saving current checkpoint @ val_loss 6.144389629364014
AllOutput: Training progress: saving best checkpoint @ val_loss 6.125506401062012
AllOutput: Training progress: saving current checkpoint @ val_loss 6.125506401062012
AllOutput: Training progress: saving best checkpoint @ val_loss 6.109384536743164
AllOutput: Training progress: saving current checkpoint @ val_loss 6.109384536743164
AllOutput: Training progress: saving current checkpoint @ val_loss 6.110946178436279
AllOutput: Training progress: saving best checkpoint @ val_loss 6.091407299041748
AllOutput: Training progress: saving current checkpoint @ val_loss 6.091407299041748
AllOutput: Training progress: saving best checkpoint @ val_loss 6.074200630187988
AllOutput: Training progress: saving current checkpoint @ val_loss 6.074200630187988
AllOutput: Training progress: saving current checkpoint @ val_loss 6.074894905090332
AllOutput: Training progress: saving best checkpoint @ val_loss 6.061723232269287
AllOutput: Training progress: saving current checkpoint @ val_loss 6.061723232269287
AllOutput: Training progress: saving best checkpoint @ val_loss 6.051541328430176
AllOutput: Training progress: saving current checkpoint @ val_loss 6.051541328430176
AllOutput: Training progress: saving best checkpoint @ val_loss 6.048675537109375
AllOutput: Training progress: saving current checkpoint @ val_loss 6.048675537109375
AllOutput: Training progress: saving best checkpoint @ val_loss 6.042313098907471
AllOutput: Training progress: saving current checkpoint @ val_loss 6.042313098907471
AllOutput: Training progress: saving best checkpoint @ val_loss 6.041115760803223
AllOutput: Training progress: saving current checkpoint @ val_loss 6.041115760803223
AllOutput: Training progress: saving best checkpoint @ val_loss 6.039181709289551
AllOutput: Training progress: saving current checkpoint @ val_loss 6.039181709289551
AllOutput: Training progress: saving best checkpoint @ val_loss 6.038957118988037
AllOutput: Training progress: saving current checkpoint @ val_loss 6.038957118988037
AllOutput: Training progress: saving best checkpoint @ val_loss 6.038074493408203
AllOutput: Training progress: saving current checkpoint @ val_loss 6.038074493408203
AllOutput: Training progress: saving best checkpoint @ val_loss 6.0374579429626465
AllOutput: Training progress: saving current checkpoint @ val_loss 6.0374579429626465
AllOutput: Training progress: saving current checkpoint @ val_loss 6.037518501281738
slurmstepd: error: *** STEP 40387993.0 ON nid008625 CANCELLED AT 2025-07-12T13:09:19 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40387993 ON nid008625 CANCELLED AT 2025-07-12T13:09:19 DUE TO TIME LIMIT ***
