# ParticleGPT #

GPT based on nanoGPT for generating particle collision data.

## Dependencies ##

https://github.com/karpathy/nanoGPT is the base project.
https://github.com/scikit-hep/particle used for PDGID conversions.
https://github.com/pybind/pybind11 used to C++ to python bindinds for pTokenizer.

## Usage ##

### Note ###

In this project:
- `real` data refers to data generated by MCGenerators using Geant4
- `generated` data refers to data generated by particleGPT.
- `tokenized` data refers to data that has been tokenized per that dataset's specifications
- `untokenized` data refers to data that was previously tokenized and has since been untokenized

### Preparing the dataset ###

- The dataset should be in `data/dataset_name.csv`.
- `dictionary.json` must exist per "preparation" and defines the tokenization for that preparation. The particles list (`particles_id` and `particles_index`) is auto populated based on information in the dataset during the preparation phase. This is to ensure only relevant particles (those present in the dataset) are included in the vocabulary.
- The preparation only needs to be done once per tokenization per dataset.
    - IMPORTANT: If there are any changes to the dataset or dictionary (for example to change the tokenization) then be sure to reprepare the data.

The preparation and dataset used in the config file will be prepared.
```shell
python prepare.py config/model_to_prepare.json
```

### Using the model ###

Training the model
```shell
# Single node, single GPU
python train.py config/model_to_train.json
# Single node, multiple GPUs (4 here)
torchrun --standalone --nproc_per_node=4 train.py config/model_to_train.json
```

Sampling
```shell
# Sampling:
python sample.py config/model_to_sample.json

# Generate distributions and calculate metrics using JetNet:
python analysis.py config/model_to_analyze.json
```

### Model outputs ###

- Trained models are stored in `trained_models/model_name/ckpt.pt`.
- Generated samples are stored in `generated_samples/model_name/sampling_index/generated_samples.csv`.
- Generated distributions are stored in `generated_samples/model_name/sampling_index/*.png` within their respective files.

## Notes ##

```shell
# Running interactive job:
srun -C "gpu" -q interactive -N 1 -G 1 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu&hbm80g" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "cpu" -q interactive -N 1 -c 128 -t 4:00:00 -A m3443 --pty /bin/bash -l

# Training on a specified GPU
CUDA_VISIBLE_DEVICES=0 python train.py config/model_to_train.json

# Profiling scripts using cProfile:
python -m cProfile -o output_file_name.profile script_to_profile.py

# Visualizing profiling using snakeviz:
# Make sure to use the port that ssh tunnels
snakeviz output_file_name.profile -p 8080 -s
```

```shell
# Dataset and num events
dataset_1 has 10,000 events
dataset_2 has 100,000 events
dataset_3 has 10,000 events
dataset_4 had 1,000,000 events
dataset_5 has 10,000,000 events
dataset_6 has 10,000 events
dataset_7 has 100,000,000 events
dataset_8 has 1,000,000,000 events
```