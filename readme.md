# ParticleGPT #

GPT based on nanoGPT for generating particle collision data.

## Dependencies ##

https://github.com/scikit-hep/particle used for PDGID conversions.

## Usage ##

### Note ###

In this project:
- `real` data refers to data generated by MCGenerators using Geant4
- `generated` data refers to data generated by particleGPT.
- `tokenized` data refers to data that has been tokenized per that dataset's specifications
- `untokenized` data refers to data that was previously tokenized and has since been untokenized

### Dataset setup ###

- The dataset should be in `data/dataset_name/data.csv`.
- `dictionary.json` must exist per dataset and defines the tokenization for that dataset. The particles list (`particles_id` and `particles_index`) is auto populated based on information in the dataset during the preparation phase. This is to ensure only relevant particles (those present in the dataset) are included in the vocabulary.
- The dataset is only prepared once. Subsequent training will just use the preprepared data.
    - IMPORTANT: If the dataset, any of the python files, or the dictionary are changed (for example to change the tokenization) then be sure to delete `outputs/meta.pkl` in the dataset directory to trigger repreparation as otherwise the dataset will not be reprepared and the old tokenized files will be reused.

### Environment setup ###

Build pTokenizerModule. This must be run for any changes to the pTokenizer code.
```shell
python setup.py
```

Prepare the dataset. This is important and must be run per dataset.
```shell
python prepare.py config/dataset_to_prepare.json
```

### Using the model ###

Training the model
```shell
# Single node, single GPU
python train.py config/dataset_to_train.json
# Single node, multiple GPU (4 here)
torchrun --standalone --nproc_per_node=4 train.py config/dataset_to_train.json 
```

Sampling
```shell
# Sampling:
python sample.py config/dataset_to_sample.json

# Generate distributions:
python generate_distributions.py config/dataset_to_use.json
```

### Model outputs ###

- Trained models are stored in `trained_models/dataset_name/ckpt.pt`.
- Generated samples are stored in `generated_samples/dataset_name/sampling_index/generated_samples.csv`.
- Generated distributions are stored in `generated_samples/dataset_name/sampling_index/` within their respective files.

## Notes ##

```shell
# Running interactive job:
srun -C "gpu" -q interactive -N 1 -G 1 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu&hbm80g" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l

# Running batch job:
python submit_job.py job_scripts/job_config_to_use.json

# Profiling scripts using cProfile:
python -m cProfile -o output_file_name.profile script_to_profile.py

# Visualizing profiling using snakeviz:
# Make sure to use the port that ssh tunnels
snakeviz output_file_name.profile -p 8080 -s
```

```shell
# Dataset and num events
dataset_1 has 10,000 events
dataset_2 has 100,000 events
dataset_3 has 10,000 events
dataset_4 had 1,000,000 events
dataset_5 has 10,000,000 events
dataset_6 has 10,000 events
```