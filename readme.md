# ParticleGPT #

GPT based on nanoGPT for generating particle collision data.

## Dependencies ##

- https://github.com/karpathy/nanoGPT is the base project.
- https://github.com/scikit-hep/particle used for PDGID conversions.
- https://github.com/scikit-hep/vector used for vector things.
- https://github.com/jet-net/JetNet/tree/main is used for numeric metrics.

## Usage ##

### Note ###

In this project:
- `real` data refers to data generated by MCGenerators using Geant4
- `generated` data refers to data generated by particleGPT.
- `tokenized` data refers to data that has been tokenized per that dataset's specifications
- `untokenized` data refers to data that was previously tokenized and has since been untokenized

### Preparing the dataset ###

- The preparation phase:
  - Tokenizes and bins the dataset so it is usable for training.
  - Creates some intermediary files for quicker analysis later.
- To prepare the data.
  1. Ensure the dataset (as a .csv file) is present in the `data/` directory.
  2. Run prepare.py with the dictionary file as an argument.
  3. IMPORTANT: The data will be prepared in the same directory as the dictionary.json file.

```shell
python prepare.py data/preparation_dir_name/dictionary.json
```

- `dictionary.json` must exist per "preparation" and defines the tokenization for that preparation. The particles list (`particles_id` and `particles_index`) is auto populated based on information in the dataset during the preparation phase. This is to ensure only relevant particles (those present in the dataset) are included in the vocabulary.
- The preparation only needs to be done once per tokenization per dataset.
    - IMPORTANT: If there are any changes to the dataset or dictionary (for example to change the tokenization) then be sure to reprepare the data.

### Using the model ###

#### Training. ####
```shell
# Single node, single GPU
python train.py config/model_to_train.json
# Single node, multiple GPUs (4 here)
torchrun --standalone --nproc_per_node=4 train.py config/model_to_train.json
```

- Trained model data will be stored in `trained_models/model_name/`.
  - Training log.
    - Stores log output as the model trains.
  - The trained model.
    - Will be stored as `ckpt.pt`.
    - Used for inference and later model usage.
  - The "running" model.
    - Will be stored as `ckpt_running.pt`.
    - This is a checkpoint stored during training used for when resuming training.
    - This is saved more often then the main model (`ckpt.pt`).

#### Inference. ####

- Inference will naturally use all available GPUs.
- GPUs can be restricted using CUDA_VISIBLE_DEVICES.

```shell
# Use all available GPUs:
python sample.py config/model_to_sample.json
# Restrict to certain GPUs (will only use GPU 0 and 2):
CUDA_VISIBLE_DEVICES=0,2 python sample.py config/model_to_sample.json
```

#### Analysis. ####

- `analysis_current.py` filters and untokenizes the data, generates distributions, and calculates numeric metrics.
- All generated data will be stored in `generated_samples/model_name/sampling_index/`

```shell
python analysis_current.py config/model_to_analyze.json
```

## Notes ##

```shell
# Running interactive job:
srun -C "gpu" -q interactive -N 1 -G 1 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "gpu&hbm80g" -q interactive -N 1 -G 4 -c 32 -t 4:00:00 -A m3443 --pty /bin/bash -l
srun -C "cpu" -q interactive -N 1 -c 128 -t 4:00:00 -A m3443 --pty /bin/bash -l

# Training on a specified GPU
CUDA_VISIBLE_DEVICES=0 python train.py config/model_to_train.json

# Profiling scripts using cProfile:
python -m cProfile -o output_file_name.profile script_to_profile.py

# Visualizing profiling using snakeviz:
# Make sure to use the port that ssh tunnels
snakeviz output_file_name.profile -p 8080 -s
```

```shell
# Dataset and num events
dataset_1 has 10,000 events
dataset_2 has 100,000 events
dataset_3 has 10,000 events
dataset_4 had 1,000,000 events
dataset_5 has 10,000,000 events
dataset_6 has 100,000,000 events
```