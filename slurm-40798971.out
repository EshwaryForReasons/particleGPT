Sat Jul 19 10:22:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:03:00.0 Off |                    0 |
| N/A   29C    P0             60W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   28C    P0             64W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:82:00.0 Off |                    0 |
| N/A   28C    P0             65W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   28C    P0             59W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
W0719 10:23:12.040000 140093089363776 torch/distributed/run.py:779] 
W0719 10:23:12.040000 140093089363776 torch/distributed/run.py:779] *****************************************
W0719 10:23:12.040000 140093089363776 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0719 10:23:12.040000 140093089363776 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Training progress: saving current checkpoint @ val_loss 4.526241302490234
AllOutput: Training progress: saving best checkpoint @ val_loss 4.469515800476074
AllOutput: Training progress: saving current checkpoint @ val_loss 4.469515800476074
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4693074226379395
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4693074226379395
AllOutput: Training progress: saving best checkpoint @ val_loss 4.469056606292725
AllOutput: Training progress: saving current checkpoint @ val_loss 4.469056606292725
AllOutput: Training progress: saving current checkpoint @ val_loss 4.469269275665283
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4685750007629395
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4685750007629395
AllOutput: Training progress: saving current checkpoint @ val_loss 4.468921184539795
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4684858322143555
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4684858322143555
AllOutput: Training progress: saving current checkpoint @ val_loss 4.468803882598877
AllOutput: Training progress: saving best checkpoint @ val_loss 4.46827507019043
AllOutput: Training progress: saving current checkpoint @ val_loss 4.46827507019043
AllOutput: Training progress: saving best checkpoint @ val_loss 4.468156337738037
AllOutput: Training progress: saving current checkpoint @ val_loss 4.468156337738037
AllOutput: Training progress: saving best checkpoint @ val_loss 4.468019008636475
AllOutput: Training progress: saving current checkpoint @ val_loss 4.468019008636475
AllOutput: Training progress: saving best checkpoint @ val_loss 4.467654228210449
AllOutput: Training progress: saving current checkpoint @ val_loss 4.467654228210449
AllOutput: Training progress: saving best checkpoint @ val_loss 4.467413425445557
AllOutput: Training progress: saving current checkpoint @ val_loss 4.467413425445557
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4671173095703125
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4671173095703125
AllOutput: Training progress: saving current checkpoint @ val_loss 4.467766761779785
AllOutput: Training progress: saving best checkpoint @ val_loss 4.466667652130127
AllOutput: Training progress: saving current checkpoint @ val_loss 4.466667652130127
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4666032791137695
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4666032791137695
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4664387702941895
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4664387702941895
AllOutput: Training progress: saving current checkpoint @ val_loss 4.466441631317139
AllOutput: Training progress: saving current checkpoint @ val_loss 4.466574668884277
AllOutput: Training progress: saving best checkpoint @ val_loss 4.466357231140137
AllOutput: Training progress: saving current checkpoint @ val_loss 4.466357231140137
AllOutput: Training progress: saving current checkpoint @ val_loss 4.467067241668701
AllOutput: Training progress: saving best checkpoint @ val_loss 4.465920448303223
AllOutput: Training progress: saving current checkpoint @ val_loss 4.465920448303223
AllOutput: Training progress: saving best checkpoint @ val_loss 4.465793609619141
AllOutput: Training progress: saving current checkpoint @ val_loss 4.465793609619141
slurmstepd: error: *** STEP 40798971.0 ON nid008564 CANCELLED AT 2025-07-21T17:22:54 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40798971 ON nid008564 CANCELLED AT 2025-07-21T17:22:54 DUE TO TIME LIMIT ***
