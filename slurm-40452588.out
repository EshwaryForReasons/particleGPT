Wed Jul  9 08:40:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:03:00.0 Off |                    0 |
| N/A   29C    P0             62W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   28C    P0             61W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:82:00.0 Off |                    0 |
| N/A   29C    P0             60W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0             58W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
W0709 08:41:10.443000 139705338046272 torch/distributed/run.py:779] 
W0709 08:41:10.443000 139705338046272 torch/distributed/run.py:779] *****************************************
W0709 08:41:10.443000 139705338046272 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0709 08:41:10.443000 139705338046272 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Training progress: saving best checkpoint @ val_loss 4.478818416595459
AllOutput: Training progress: saving current checkpoint @ val_loss 4.478818416595459
AllOutput: Training progress: saving best checkpoint @ val_loss 4.478790283203125
AllOutput: Training progress: saving current checkpoint @ val_loss 4.478790283203125
AllOutput: Training progress: saving best checkpoint @ val_loss 4.478641033172607
AllOutput: Training progress: saving current checkpoint @ val_loss 4.478641033172607
AllOutput: Training progress: saving best checkpoint @ val_loss 4.478281497955322
AllOutput: Training progress: saving current checkpoint @ val_loss 4.478281497955322
AllOutput: Training progress: saving best checkpoint @ val_loss 4.477926731109619
AllOutput: Training progress: saving current checkpoint @ val_loss 4.477926731109619
AllOutput: Training progress: saving best checkpoint @ val_loss 4.477356433868408
AllOutput: Training progress: saving current checkpoint @ val_loss 4.477356433868408
AllOutput: Training progress: saving best checkpoint @ val_loss 4.477242946624756
AllOutput: Training progress: saving current checkpoint @ val_loss 4.477242946624756
AllOutput: Training progress: saving best checkpoint @ val_loss 4.476871490478516
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476871490478516
AllOutput: Training progress: saving best checkpoint @ val_loss 4.476581573486328
AllOutput: Training progress: saving current checkpoint @ val_loss 4.476581573486328
AllOutput: Training progress: saving best checkpoint @ val_loss 4.475971698760986
AllOutput: Training progress: saving current checkpoint @ val_loss 4.475971698760986
AllOutput: Training progress: saving best checkpoint @ val_loss 4.474966526031494
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474966526031494
slurmstepd: error: *** STEP 40452588.0 ON nid008673 CANCELLED AT 2025-07-10T13:40:57 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40452588 ON nid008673 CANCELLED AT 2025-07-10T13:40:57 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
