{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#db7d60\">Setup</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pUtil\n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines functions used across the whole of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data_columns():\n",
    "    return [\n",
    "        'model_name',\n",
    "        # Metadata (dataset information)\n",
    "        'vocab_size',\n",
    "        'max_sequence_length',\n",
    "        'num_train_tokens',\n",
    "        'num_val_tokens',\n",
    "        # Configuration information\n",
    "        'batch_size',\n",
    "        'learning_rate',\n",
    "        'min_lr',\n",
    "        'lr_decay_iters',\n",
    "        'n_layer',\n",
    "        'n_head',\n",
    "        'n_embd',\n",
    "        'scheme',\n",
    "        'preparation_name',\n",
    "        # Training information\n",
    "        \"iters_trained\",\n",
    "        'min_saved_train_loss',\n",
    "        'min_saved_val_loss'\n",
    "    ]\n",
    "\n",
    "def get_default_df(model_names):\n",
    "    columns = get_model_data_columns()\n",
    "    model_data_list = [row for name in model_names if (row := get_model_data_for_model(name)) is not None]\n",
    "    model_data_df = pd.DataFrame(model_data_list, columns=columns)\n",
    "    return model_data_df\n",
    "\n",
    "# iterations_per_epoch must be provided for epoch calculations\n",
    "def get_train_data_for_model(model_name, iterations_per_epoch=-1):\n",
    "    training_log_filename = pUtil.get_training_dir(model_name) / \"train_log_1.jsonl\"\n",
    "    \n",
    "    # Training information\n",
    "    running_data, saved_data = [], []\n",
    "    with open(training_log_filename) as training_log_file:\n",
    "        for jline in training_log_file:\n",
    "            jdata = json.loads(jline)\n",
    "            if jdata.get(\"message\") == \"Training progress\" and \"iter\" in jdata:\n",
    "                current_epochs_trained = 0 if jdata['iter'] == 0 else (jdata['iter'] / iterations_per_epoch)\n",
    "                running_data.append({'iter': jdata[\"iter\"], 'epoch': current_epochs_trained, 'train_loss': jdata[\"train_loss\"], 'val_loss': jdata[\"val_loss\"]})\n",
    "            elif jdata.get(\"message\") == \"Training progress: checking checkpoint conditions\":\n",
    "                current_epochs_trained = 0 if jdata['step'] == 0 else (jdata['step'] / iterations_per_epoch)\n",
    "                saved_data.append({'iter': jdata[\"step\"], 'epoch': current_epochs_trained, 'train_loss': jdata[\"train_loss\"], 'val_loss': jdata[\"val_loss\"]})\n",
    "    \n",
    "    return running_data, saved_data\n",
    "\n",
    "def get_model_data_for_model(model_name):\n",
    "    meta_filename = pUtil.get_model_meta_filename(model_name)\n",
    "    config_filename = pUtil.get_model_config_filename(model_name)\n",
    "    \n",
    "    if not meta_filename.exists():\n",
    "        return None\n",
    "    \n",
    "    # Metadata (dataset information)\n",
    "    with open(meta_filename, 'rb') as meta_file:\n",
    "        meta_data = pickle.load(meta_file)\n",
    "        vocab_size = meta_data[\"vocab_size\"]\n",
    "        max_sequence_length = meta_data['max_sequence_length']\n",
    "        num_train_tokens = meta_data['num_train_tokens']\n",
    "        num_train_tokens = meta_data['num_val_tokens']\n",
    "\n",
    "    # Configuration information\n",
    "    with open(config_filename, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        training_config = config.get('training_config', {})\n",
    "        batch_size = training_config.get('batch_size', -1)\n",
    "        block_size = training_config.get('block_size', -1)\n",
    "        context_events = training_config.get('context_events', -1)\n",
    "        learning_rate = training_config.get('learning_rate', -1)\n",
    "        min_lr = training_config.get('min_lr', -1)\n",
    "        lr_decay_iters = training_config.get('lr_decay_iters', -1)\n",
    "        n_layer = training_config.get('n_layer', -1)\n",
    "        n_head = training_config.get('n_head', -1)\n",
    "        n_embd = training_config.get('n_embd', -1)\n",
    "        scheme = training_config.get('scheme', 'standard')\n",
    "        preparation_name = training_config.get('preparation_name', 'unknown')\n",
    "        \n",
    "        if block_size == -1:\n",
    "            block_size = context_events * max_sequence_length\n",
    "        \n",
    "        iterations_per_epoch = num_train_tokens // (batch_size * block_size)\n",
    "    \n",
    "    # Training information\n",
    "    running_data, saved_data = get_train_data_for_model(model_name, iterations_per_epoch)                \n",
    "    running_df = pd.DataFrame(running_data)\n",
    "    saved_df = pd.DataFrame(saved_data)\n",
    "    \n",
    "    iters_trained = running_df['iter'].max()\n",
    "    min_saved_val_loss_row = saved_df.loc[saved_df['val_loss'].idxmin()]\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        # Metadata (dataset information)\n",
    "        'vocab_size': vocab_size,\n",
    "        'max_sequence_length': max_sequence_length,\n",
    "        'num_train_tokens': num_train_tokens,\n",
    "        'num_val_tokens': num_train_tokens,\n",
    "        # Configuration information\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_lr': min_lr,\n",
    "        'lr_decay_iters': lr_decay_iters,\n",
    "        'n_layer': n_layer,\n",
    "        'n_head': n_head,\n",
    "        'n_embd': n_embd,\n",
    "        'scheme': scheme,\n",
    "        'preparation_name': preparation_name,\n",
    "        # Training information\n",
    "        \"iters_trained\": iters_trained,\n",
    "        'min_saved_train_loss': min_saved_val_loss_row['train_loss'],\n",
    "        'min_saved_val_loss': min_saved_val_loss_row['val_loss']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_train_info(model_name, ax=None, use_epochs=True, y_lim=None, use_log_scale=True):\n",
    "    training_log_filename = pUtil.get_training_dir(model_name) / \"train_log_1.jsonl\"\n",
    "    meta_filename = pUtil.get_model_meta_filename(model_name)\n",
    "    config_filename = pUtil.get_model_config_filename(model_name)\n",
    "    \n",
    "    with open(meta_filename, 'rb') as meta_file:\n",
    "        meta_data = pickle.load(meta_file)\n",
    "        max_sequence_length = meta_data['max_sequence_length']\n",
    "        num_train_tokens = meta_data['num_train_tokens']\n",
    "\n",
    "    with open(config_filename, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        training_config = config.get('training_config', {})\n",
    "        batch_size = training_config.get('batch_size', -1)\n",
    "        block_size = training_config.get('block_size', -1)\n",
    "        context_events = training_config.get('context_events', -1)\n",
    "        \n",
    "        if block_size == -1:\n",
    "            block_size = context_events * max_sequence_length\n",
    "        \n",
    "        iterations_per_epoch = num_train_tokens // (batch_size * block_size)\n",
    "\n",
    "    running_data, saved_data = {}, {}\n",
    "    with open(training_log_filename) as training_log_file:\n",
    "        for jline in training_log_file:\n",
    "            jdata = json.loads(jline)\n",
    "            if jdata.get(\"message\") == \"Training progress\" and \"iter\" in jdata:\n",
    "                current_epochs_trained = 0 if jdata['iter'] == 0 else (jdata['iter'] / iterations_per_epoch)\n",
    "                running_data[jdata[\"iter\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "            elif jdata.get(\"message\") == \"Training progress: checking checkpoint conditions\":\n",
    "                current_epochs_trained = 0 if jdata['step'] == 0 else (jdata['step'] / iterations_per_epoch)\n",
    "                saved_data[jdata[\"step\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "\n",
    "    sorted_running_iters = sorted(running_data)\n",
    "    running_epochs_trained, running_train_loss, running_val_loss = zip(*(running_data[i] for i in sorted_running_iters))\n",
    "\n",
    "    saved_saved_iters = sorted(saved_data)\n",
    "    saved_epochs_trained, saved_train_loss, saved_val_loss = zip(*(saved_data[i] for i in saved_saved_iters))\n",
    "\n",
    "    # Find min loss points\n",
    "    min_val_loss_iter_idx = saved_val_loss.index(min(saved_val_loss))\n",
    "    min_saved_losses_iter = saved_saved_iters[min_val_loss_iter_idx]\n",
    "    min_saved_val_loss_epoch, min_saved_train_loss, min_saved_val_loss = saved_epochs_trained[min_val_loss_iter_idx], saved_train_loss[min_val_loss_iter_idx], saved_val_loss[min_val_loss_iter_idx]\n",
    "    \n",
    "    ax = ax or plt\n",
    "    if use_epochs:\n",
    "        train_plot_line, = ax.plot(running_epochs_trained, running_train_loss, label=f'Training Loss ({model_name})')\n",
    "        val_plot_line, = ax.plot(running_epochs_trained, running_val_loss, label=f'Validation Loss ({model_name})', color=train_plot_line.get_color(), linestyle='--')\n",
    "        ax.scatter(min_saved_val_loss_epoch, min_saved_train_loss, label=f'Min Train Loss ({model_name}; ${min_saved_train_loss:.4f}$ @ epoch {min_saved_val_loss_epoch:.5f})', color=train_plot_line.get_color(), edgecolors='black')\n",
    "        ax.scatter(min_saved_val_loss_epoch, min_saved_val_loss, label=f'Min Val Loss ({model_name}; ${min_saved_val_loss:.4f}$ @ epoch {min_saved_val_loss_epoch:.5f})', color=train_plot_line.get_color(), edgecolors='black')\n",
    "        if ax is not plt:\n",
    "            ax.set_xlim([0, max(running_epochs_trained)])\n",
    "            if y_lim is not None:\n",
    "                ax.set_ylim(y_lim)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            ax.set_ylabel('Loss')\n",
    "            if use_log_scale:\n",
    "                ax.set_yscale('log', base=10)\n",
    "            ax.set_title(f'{model_name}')\n",
    "            ax.legend()\n",
    "    else:\n",
    "        train_plot_line, = ax.plot(sorted_running_iters, running_train_loss, label=f'Training Loss ({model_name})', linewidth=0.5)\n",
    "        val_plot_line, = ax.plot(sorted_running_iters, running_val_loss, label=f'Validation Loss ({model_name})', color=train_plot_line.get_color(), linestyle='--', linewidth=0.5)\n",
    "        ax.scatter(min_saved_losses_iter, min_saved_val_loss, label=f'Min Val Loss ({model_name}; ${min_saved_val_loss:.4f}$ @ iter {min_saved_losses_iter})', color=train_plot_line.get_color(), edgecolors='black', marker='s', s=50, )\n",
    "        ax.scatter(min_saved_losses_iter, min_saved_train_loss, label=f'Min Train Loss ({model_name}; ${min_saved_train_loss:.4f}$ @ iter {min_saved_losses_iter})', color=train_plot_line.get_color(), edgecolors='black')\n",
    "        if ax is not plt:\n",
    "            if y_lim is not None:\n",
    "                ax.set_ylim(y_lim)\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel('Loss')\n",
    "            if use_log_scale:\n",
    "                ax.set_yscale('log', base=10)\n",
    "            ax.set_title(f'{model_name}')\n",
    "            ax.legend()\n",
    "    \n",
    "    x_offset = 0.005 * sorted_running_iters[-1]\n",
    "    y_offset = 0.02\n",
    "    # ax.annotate(model_name, xy=(sorted_running_iters[-1], running_train_loss[-1]), xytext=(sorted_running_iters[-1] + x_offset, running_train_loss[-1] + y_offset), fontsize=9, color=train_plot_line.get_color())\n",
    "    ax.annotate(model_name, xy=(sorted_running_iters[-1], running_val_loss[-1]), xytext=(sorted_running_iters[-1] + x_offset, running_val_loss[-1] - y_offset), fontsize=9, color=val_plot_line.get_color())\n",
    "    \n",
    "def plot_train_graphs(models_to_compare, juxtaposed=True, use_epochs=True, y_lim=None, use_log_scale=True):\n",
    "    if juxtaposed:\n",
    "        num_horizontal, num_vertical = min(len(models_to_compare), 3), (math.ceil(len(models_to_compare) / 3))\n",
    "        figure, axes = plt.subplots(num_vertical, num_horizontal, figsize=(8 * num_horizontal, 6 * num_vertical), sharex=False, sharey=True)\n",
    "        if len(models_to_compare) == 1:\n",
    "            axes = [axes]\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "        for model_name, ax in zip(models_to_compare, axes):\n",
    "            plot_model_train_info(model_name, ax=ax, use_epochs=use_epochs, y_lim=y_lim, use_log_scale=use_log_scale)\n",
    "        figure.suptitle(f'Training Progress for {\", \".join(models_to_compare)}')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        for model_name in models_to_compare:\n",
    "            plot_model_train_info(model_name, use_epochs=use_epochs, y_lim=y_lim, use_log_scale=use_log_scale)\n",
    "        wrapped_title = \"\\n\".join(textwrap.wrap(f'Training Progress for {\", \".join(models_to_compare)}', width=60))\n",
    "        plt.title(wrapped_title)\n",
    "        if use_log_scale:\n",
    "            plt.yscale('log', base=10)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        if y_lim is not None:\n",
    "            plt.ylim(y_lim)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"num_particles\", \"pdgid\", \"e\", \"px\", \"py\", \"pz\", \"eta\", \"theta\", \"phi\"]\n",
    "\n",
    "def get_common_data(model_name):\n",
    "    dictionary_filename = pUtil.get_model_preparation_dir(model_name) / 'dictionary.json'\n",
    "    real_leading_test_particles_filename = pUtil.get_model_preparation_dir(model_name) / 'real_leading_test_particles.csv'\n",
    "    sampled_leading_particles_filename = pUtil.get_latest_sampling_dir(model_name) / 'sampled_leading_particles.csv'\n",
    "\n",
    "    with open(dictionary_filename) as dictionary_file:\n",
    "        dictionary = json.load(dictionary_file)\n",
    "\n",
    "    # Convenience dictionary definitions\n",
    "    p_bin_count = (dictionary[\"e_bin_data\"][\"max\"] - dictionary[\"e_bin_data\"][\"min\"]) // 1000\n",
    "    e_bin_count = (dictionary[\"e_bin_data\"][\"max\"] - dictionary[\"e_bin_data\"][\"min\"]) // dictionary[\"e_bin_data\"][\"step_size\"]\n",
    "    eta_bin_count = int((dictionary[\"eta_bin_data\"][\"max\"] - dictionary[\"eta_bin_data\"][\"min\"]) // dictionary[\"eta_bin_data\"][\"step_size\"])\n",
    "            \n",
    "    bin_settings = {\n",
    "        \"num_particles\": { \"min\": 0,                                 \"max\": 50,                                \"bins\": 50 },\n",
    "        \"e\":             { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": e_bin_count },\n",
    "        \"px\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"py\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"pz\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"eta\":           { \"min\": dictionary[\"eta_bin_data\"][\"min\"], \"max\": dictionary[\"eta_bin_data\"][\"max\"], \"bins\": eta_bin_count },\n",
    "        \"theta\":         { \"min\": dictionary[\"theta_bin_data\"][\"min\"], \"max\": dictionary[\"theta_bin_data\"][\"max\"], \"bins\": int((4 * np.pi) // dictionary[\"theta_bin_data\"][\"step_size\"]) },\n",
    "        \"phi\":           { \"min\": dictionary[\"phi_bin_data\"][\"min\"], \"max\": dictionary[\"phi_bin_data\"][\"max\"], \"bins\": int((4 * np.pi) // dictionary[\"phi_bin_data\"][\"step_size\"]) },\n",
    "    }\n",
    "\n",
    "    df1 = pd.read_csv(real_leading_test_particles_filename, sep=\" \", names=columns, engine=\"c\", header=None)\n",
    "    df2 = pd.read_csv(sampled_leading_particles_filename, sep=\" \", names=columns, engine=\"c\", header=None)\n",
    "    return bin_settings, df1, df2\n",
    "\n",
    "def generate_distributions(model_name, column_name, ax=None):\n",
    "    bin_settings, df1, df2 = get_common_data(model_name)\n",
    "    \n",
    "    min_val = bin_settings[column_name]['min']\n",
    "    max_val = bin_settings[column_name]['max']\n",
    "    bins = bin_settings[column_name]['bins']\n",
    "    \n",
    "    df1_weights = np.ones_like(df1[column_name]) / len(df1[column_name])\n",
    "    df2_weights = np.ones_like(df2[column_name]) / len(df2[column_name])\n",
    "    \n",
    "    ax = ax or plt\n",
    "    ax.hist(df1[column_name], bins=bins, weights=df1_weights, range=(min_val, max_val), alpha=0.7, color=\"blue\", label=f'Input ({model_name})')\n",
    "    ax.hist(df2[column_name], bins=bins, weights=df2_weights, range=(min_val, max_val), alpha=0.7, color=\"orange\", label=f'Sampled ({model_name})')\n",
    "    if ax is not plt:\n",
    "        ax.set_xlabel(column_name)\n",
    "        ax.set_ylabel('Frequency (Normalized)')\n",
    "        ax.set_title(f'{model_name}')\n",
    "        ax.legend()\n",
    "\n",
    "def compare_distributions(models_to_compare, column_name, juxtaposed=True, dists_per_row=3):\n",
    "    if juxtaposed:\n",
    "        num_horizontal, num_vertical = min(len(models_to_compare), dists_per_row), (math.ceil(len(models_to_compare) / dists_per_row))\n",
    "        figure, axes = plt.subplots(num_vertical, num_horizontal, figsize=(8 * num_horizontal, 6 * num_vertical), sharex=False, sharey=True)\n",
    "        if len(models_to_compare) == 1:\n",
    "            axes = [axes]\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "        for model_name, ax in zip(models_to_compare, axes):\n",
    "            generate_distributions(model_name, column_name=column_name, ax=ax)\n",
    "        figure.suptitle(f'Training Progress for {\", \".join(models_to_compare)}')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        for model_name in models_to_compare:\n",
    "            generate_distributions(model_name, column_name=column_name)\n",
    "        plt.title(f'Training Progress for {\", \".join(models_to_compare)}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#db7d60\">Comparisions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_columns():\n",
    "    return [\n",
    "        'model_name',\n",
    "        # Metadata (dataset information)\n",
    "        'vocab_size',\n",
    "        'max_sequence_length',\n",
    "        'num_train_tokens',\n",
    "        'num_val_tokens',\n",
    "        # Configuration information\n",
    "        'batch_size',\n",
    "        'learning_rate',\n",
    "        'min_lr',\n",
    "        'lr_decay_iters',\n",
    "        'n_layer',\n",
    "        'n_head',\n",
    "        'n_embd',\n",
    "        'scheme',\n",
    "        'preparation_name',\n",
    "        # Training information\n",
    "        \"iters_trained\",\n",
    "        'min_saved_train_loss',\n",
    "        'min_saved_val_loss'\n",
    "    ]\n",
    "\n",
    "def get_default_df(model_names):\n",
    "    columns = get_train_data_columns()\n",
    "    model_data_list = [row for name in model_names if (row := get_train_data_as_row(name)) is not None]\n",
    "    model_data_df = pd.DataFrame(model_data_list, columns=columns)\n",
    "    return model_data_df\n",
    "\n",
    "def get_train_data_as_row(model_name):\n",
    "    training_log_filename = pUtil.get_training_dir(model_name) / \"train_log_1.jsonl\"\n",
    "    meta_filename = pUtil.get_model_meta_filename(model_name)\n",
    "    config_filename = pUtil.get_model_config_filename(model_name)\n",
    "    \n",
    "    if not meta_filename.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(meta_filename, 'rb') as meta_file:\n",
    "        meta_data = pickle.load(meta_file)\n",
    "        vocab_size = meta_data[\"vocab_size\"]\n",
    "        max_sequence_length = meta_data['max_sequence_length']\n",
    "        num_train_tokens = meta_data['num_train_tokens']\n",
    "        num_train_tokens = meta_data['num_val_tokens']\n",
    "\n",
    "    with open(config_filename, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        training_config = config.get('training_config', {})\n",
    "        batch_size = training_config.get('batch_size', -1)\n",
    "        block_size = training_config.get('block_size', -1)\n",
    "        context_events = training_config.get('context_events', -1)\n",
    "        learning_rate = training_config.get('learning_rate', -1)\n",
    "        min_lr = training_config.get('min_lr', -1)\n",
    "        lr_decay_iters = training_config.get('lr_decay_iters', -1)\n",
    "        n_layer = training_config.get('n_layer', -1)\n",
    "        n_head = training_config.get('n_head', -1)\n",
    "        n_embd = training_config.get('n_embd', -1)\n",
    "        scheme = training_config.get('scheme', 'standard')\n",
    "        preparation_name = training_config.get('preparation_name', 'unknown')\n",
    "        \n",
    "        if block_size == -1:\n",
    "            block_size = context_events * max_sequence_length\n",
    "        \n",
    "        iterations_per_epoch = num_train_tokens // (batch_size * block_size)\n",
    "        \n",
    "    running_data, saved_data = [], []\n",
    "    with open(training_log_filename) as training_log_file:\n",
    "        for jline in training_log_file:\n",
    "            jdata = json.loads(jline)\n",
    "            if jdata.get(\"message\") == \"Training progress\" and \"iter\" in jdata:\n",
    "                current_epochs_trained = 0 if jdata['iter'] == 0 else (jdata['iter'] / iterations_per_epoch)\n",
    "                running_data.append({'iter': jdata[\"iter\"], 'epoch': current_epochs_trained, 'train_loss': jdata[\"train_loss\"], 'val_loss': jdata[\"val_loss\"]})\n",
    "            elif jdata.get(\"message\") == \"Training progress: checking checkpoint conditions\":\n",
    "                current_epochs_trained = 0 if jdata['step'] == 0 else (jdata['step'] / iterations_per_epoch)\n",
    "                saved_data.append({'iter': jdata[\"step\"], 'epoch': current_epochs_trained, 'train_loss': jdata[\"train_loss\"], 'val_loss': jdata[\"val_loss\"]})\n",
    "                \n",
    "    running_df = pd.DataFrame(running_data)\n",
    "    saved_df = pd.DataFrame(saved_data)\n",
    "    \n",
    "    iters_trained = running_df['iter'].max()\n",
    "    min_saved_val_loss_row = saved_df.loc[saved_df['val_loss'].idxmin()]\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        # Metadata (dataset information)\n",
    "        'vocab_size': vocab_size,\n",
    "        'max_sequence_length': max_sequence_length,\n",
    "        'num_train_tokens': num_train_tokens,\n",
    "        'num_val_tokens': num_train_tokens,\n",
    "        # Configuration information\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_lr': min_lr,\n",
    "        'lr_decay_iters': lr_decay_iters,\n",
    "        'n_layer': n_layer,\n",
    "        'n_head': n_head,\n",
    "        'n_embd': n_embd,\n",
    "        'scheme': scheme,\n",
    "        'preparation_name': preparation_name,\n",
    "        # Training information\n",
    "        \"iters_trained\": iters_trained,\n",
    "        'min_saved_train_loss': min_saved_val_loss_row['train_loss'],\n",
    "        'min_saved_val_loss': min_saved_val_loss_row['val_loss']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing different learning rates and their effect on validation loss and the model distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_for_model(model_name, ax=None, use_epochs=True):\n",
    "    training_log_filename = pUtil.get_training_dir(model_name) / \"train_log_1.jsonl\"\n",
    "    meta_filename = pUtil.get_model_meta_filename(model_name)\n",
    "    config_filename = pUtil.get_model_config_filename(model_name)\n",
    "    \n",
    "    with open(meta_filename, 'rb') as meta_file:\n",
    "        meta_data = pickle.load(meta_file)\n",
    "        max_sequence_length = meta_data['max_sequence_length']\n",
    "        num_train_tokens = meta_data['num_train_tokens']\n",
    "\n",
    "    with open(config_filename, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        training_config = config.get('training_config', {})\n",
    "        batch_size = training_config.get('batch_size', -1)\n",
    "        block_size = training_config.get('block_size', -1)\n",
    "        context_events = training_config.get('context_events', -1)\n",
    "        learning_rate = training_config.get('learning_rate', -1)\n",
    "        lr_decay_iters = training_config.get('lr_decay_iters', -1)\n",
    "        min_lr = training_config.get('min_lr', -1)\n",
    "        \n",
    "        if block_size == -1:\n",
    "            block_size = context_events * max_sequence_length\n",
    "        \n",
    "        iterations_per_epoch = num_train_tokens // (batch_size * block_size)\n",
    "\n",
    "    running_data, saved_data = {}, {}\n",
    "    with open(training_log_filename) as training_log_file:\n",
    "        for jline in training_log_file:\n",
    "            jdata = json.loads(jline)\n",
    "            if jdata.get(\"message\") == \"Training progress\" and \"iter\" in jdata:\n",
    "                current_epochs_trained = 0 if jdata['iter'] == 0 else (jdata['iter'] / iterations_per_epoch)\n",
    "                running_data[jdata[\"iter\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "            elif jdata.get(\"message\") == \"Training progress: checking checkpoint conditions\":\n",
    "                current_epochs_trained = 0 if jdata['step'] == 0 else (jdata['step'] / iterations_per_epoch)\n",
    "                saved_data[jdata[\"step\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "\n",
    "    sorted_running_iters = sorted(running_data)\n",
    "    running_epochs_trained, running_train_loss, running_val_loss = zip(*(running_data[i] for i in sorted_running_iters))\n",
    "\n",
    "    saved_saved_iters = sorted(saved_data)\n",
    "    saved_epochs_trained, saved_train_loss, saved_val_loss = zip(*(saved_data[i] for i in saved_saved_iters))\n",
    "\n",
    "    # Find min loss points\n",
    "    min_val_loss_iter_idx = saved_val_loss.index(min(saved_val_loss))\n",
    "    min_saved_losses_iter = saved_saved_iters[min_val_loss_iter_idx]\n",
    "    min_saved_val_loss_epoch, min_saved_train_loss, min_saved_val_loss = saved_epochs_trained[min_val_loss_iter_idx], saved_train_loss[min_val_loss_iter_idx], saved_val_loss[min_val_loss_iter_idx]\n",
    "\n",
    "    return learning_rate, min_lr, lr_decay_iters, min_saved_train_loss, min_saved_val_loss\n",
    "\n",
    "def plot_learning_rate_effects(model_information_dict):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    learning_rates = []\n",
    "    min_lrs = []\n",
    "    val_losses = []\n",
    "    labels = []\n",
    "    for model_name, info in model_information_dict.items():\n",
    "        learning_rates.append(info['learning_rate'])\n",
    "        min_lrs.append(info['min_lr'])\n",
    "        val_losses.append(info['min_saved_val_loss'])\n",
    "        labels.append(model_name)\n",
    "    \n",
    "     # Identify the best (lowest) original validation loss\n",
    "    raw_val_losses = [info['min_saved_val_loss'] for info in model_information_dict.values() if info['learning_rate'] > 0 and info['min_lr'] > 0 and info['min_saved_val_loss'] > 0]\n",
    "    min_idx = np.argmin(raw_val_losses)\n",
    "    \n",
    "    best_x = learning_rates[min_idx]\n",
    "    best_y = min_lrs[min_idx]\n",
    "    best_z = val_losses[min_idx]\n",
    "    best_label = labels[min_idx]\n",
    "\n",
    "    scatter = ax.scatter(learning_rates, min_lrs, val_losses, c=val_losses, cmap='viridis', s=60)\n",
    "    ax.scatter([best_x], [best_y], [best_z], color='red', s=100, label=f'Best Model: {best_label}')\n",
    "    \n",
    "    for lr, mlr, vl, label in zip(learning_rates, min_lrs, val_losses, labels):\n",
    "        ax.text(lr, mlr, vl, label, size=8)\n",
    "\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Min LR\")\n",
    "    ax.set_zlabel(\"Min Validation Loss\")\n",
    "    ax.set_title(\"Effect of Learning Rates on Min Validation Loss\")\n",
    "    fig.colorbar(scatter, ax=ax, label='Min Val Loss')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_rate_effects_log10(model_information_dict):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    learning_rates = []\n",
    "    min_lrs = []\n",
    "    val_losses = []\n",
    "    labels = []\n",
    "    for model_name, info in model_information_dict.items():\n",
    "        lr = info['learning_rate']\n",
    "        mlr = info['min_lr']\n",
    "        vl = info['min_saved_val_loss']\n",
    "\n",
    "        # Skip invalid or non-positive values\n",
    "        if lr > 0 and mlr > 0 and vl > 0:\n",
    "            learning_rates.append(np.log10(lr))\n",
    "            min_lrs.append(np.log10(mlr))\n",
    "            val_losses.append(np.log10(vl))\n",
    "            labels.append(model_name)\n",
    "\n",
    "    # Identify the best (lowest) original validation loss\n",
    "    raw_val_losses = [info['min_saved_val_loss'] for info in model_information_dict.values() if info['learning_rate'] > 0 and info['min_lr'] > 0 and info['min_saved_val_loss'] > 0]\n",
    "    min_idx = np.argmin(raw_val_losses)\n",
    "    \n",
    "    best_x = learning_rates[min_idx]\n",
    "    best_y = min_lrs[min_idx]\n",
    "    best_z = val_losses[min_idx]\n",
    "    best_label = labels[min_idx]\n",
    "\n",
    "    scatter = ax.scatter(learning_rates, min_lrs, val_losses, c=val_losses, cmap='viridis', s=60)\n",
    "\n",
    "    # Highlight the best point\n",
    "    ax.scatter([best_x], [best_y], [best_z], color='red', s=100, label=f'Best Model: {best_label}')\n",
    "\n",
    "    for x, y, z, label in zip(learning_rates, min_lrs, val_losses, labels):\n",
    "        ax.text(x, y, z, label, size=8)\n",
    "\n",
    "    # Axis labels in log10 terms\n",
    "    ax.set_xlabel(\"log10(Learning Rate)\")\n",
    "    ax.set_ylabel(\"log10(Min LR)\")\n",
    "    ax.set_zlabel(\"log10(Min Validation Loss)\")\n",
    "    ax.set_title(\"Effect of Learning Rates on Min Validation Loss\")\n",
    "    fig.colorbar(scatter, ax=ax, label='log10(Min Val Loss)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_model_info_table(model_information_dict):\n",
    "    # Sort model data by min_saved_val_loss\n",
    "    sorted_items = sorted(\n",
    "        model_information_dict.items(),\n",
    "        key=lambda x: x[1]['min_saved_val_loss']\n",
    "    )\n",
    "\n",
    "    # Prepare table data\n",
    "    columns = [\"Model Name\", \"Learning Rate\", \"Min LR\", \"Decay Iters\", \"Avg Learning Rate\", \"Min Val Loss\"]\n",
    "    cell_data = [\n",
    "        [\n",
    "            model_name,\n",
    "            f\"{info['learning_rate']:.2e}\",\n",
    "            f\"{info['min_lr']:.2e}\",\n",
    "            f\"{info['lr_decay_iters']:.5f}\",\n",
    "            f\"{(info['learning_rate'] - info['min_lr']) / info['lr_decay_iters']:.2e}\",\n",
    "            f\"{info['min_saved_val_loss']:.5f}\"\n",
    "        ]\n",
    "        for model_name, info in sorted_items\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, len(cell_data) * 0.4 + 1))\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=cell_data,\n",
    "        colLabels=columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)\n",
    "    plt.title(\"Model Training Summary (Sorted by Min Val Loss)\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "models_to_compare = ['model_5_3_1', 'model_5_3_2', 'model_5_3_3', 'model_5_3_4', 'model_5_3_8', \n",
    "                     'model_5_3_9', 'model_5_3_10', 'model_5_3_11', 'model_5_3_12', 'model_5_3_13', \n",
    "                     'model_5_3_14', 'model_5_3_15', 'model_5_3_16', 'model_5_3_17', 'model_5_3_18']\n",
    "model_information_dict = {}\n",
    "for model_name in models_to_compare:\n",
    "    learning_rate, min_lr, lr_decay_iters, min_saved_train_loss, min_saved_val_loss = get_train_data_for_model(model_name)\n",
    "    model_information_dict[model_name] = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"min_lr\": min_lr,\n",
    "        \"lr_decay_iters\": lr_decay_iters,\n",
    "        \"min_saved_train_loss\": min_saved_train_loss,\n",
    "        \"min_saved_val_loss\": min_saved_val_loss,\n",
    "    }\n",
    "\n",
    "summary_stats = {}\n",
    "value_keys = next(iter(model_information_dict.values())).keys()\n",
    "for key in value_keys:\n",
    "    all_values = [info[key] for info in model_information_dict.values()]\n",
    "    summary_stats[key] = {\n",
    "        'min': min(all_values),\n",
    "        'max': max(all_values)\n",
    "    }\n",
    "\n",
    "for key, stats in summary_stats.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Min: {stats['min']}\")\n",
    "    print(f\"  Max: {stats['max']}\")\n",
    "\n",
    "models_to_compare = ['model_5_3_1', 'model_5_3_2', 'model_5_3_3', 'model_5_3_4', 'model_5_3_5', 'model_5_3_6', 'model_5_3_7', 'model_5_3_8', 'model_5_3_9', 'model_5_3_10', 'model_5_3_11', 'model_5_3_12',]\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)\n",
    "models_to_compare = ['model_5_3_9', 'model_5_3_10', 'model_5_3_12']\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, y_lim=[0.78, 0.85], use_log_scale=True)\n",
    "\n",
    "print(json.dumps(model_information_dict, indent=4))\n",
    "create_model_info_table(model_information_dict)\n",
    "plot_learning_rate_effects(model_information_dict)\n",
    "plot_learning_rate_effects_log10(model_information_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = ['model_5_2_10', 'model_5_2_8', 'model_5_2_7', 'model_5_2_6', 'model_5_2_5', 'model_5_2_4', 'model_5_2_3', 'model_5_2_2', 'model_5_2_1']\n",
    "for column in columns:\n",
    "    if column == 'pdgid':\n",
    "        continue\n",
    "    compare_distributions(models_to_compare, column_name=column, juxtaposed=True, dists_per_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing different number of layers and their effect on validation loss and model distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_for_model(model_name, ax=None, use_epochs=True):\n",
    "    training_log_filename = pUtil.get_training_dir(model_name) / \"train_log_1.jsonl\"\n",
    "    meta_filename = pUtil.get_model_meta_filename(model_name)\n",
    "    config_filename = pUtil.get_model_config_filename(model_name)\n",
    "    \n",
    "    with open(meta_filename, 'rb') as meta_file:\n",
    "        meta_data = pickle.load(meta_file)\n",
    "        max_sequence_length = meta_data['max_sequence_length']\n",
    "        num_train_tokens = meta_data['num_train_tokens']\n",
    "\n",
    "    with open(config_filename, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        training_config = config.get('training_config', {})\n",
    "        batch_size = training_config.get('batch_size', -1)\n",
    "        block_size = training_config.get('block_size', -1)\n",
    "        context_events = training_config.get('context_events', -1)\n",
    "        learning_rate = training_config.get('learning_rate', -1)\n",
    "        lr_decay_iters = training_config.get('lr_decay_iters', -1)\n",
    "        num_layers = training_config.get('n_layer', -1)\n",
    "        min_lr = training_config.get('min_lr', -1)\n",
    "        \n",
    "        if block_size == -1:\n",
    "            block_size = context_events * max_sequence_length\n",
    "        \n",
    "        iterations_per_epoch = num_train_tokens // (batch_size * block_size)\n",
    "\n",
    "    running_data, saved_data = {}, {}\n",
    "    with open(training_log_filename) as training_log_file:\n",
    "        for jline in training_log_file:\n",
    "            jdata = json.loads(jline)\n",
    "            if jdata.get(\"message\") == \"Training progress\" and \"iter\" in jdata:\n",
    "                current_epochs_trained = 0 if jdata['iter'] == 0 else (jdata['iter'] / iterations_per_epoch)\n",
    "                running_data[jdata[\"iter\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "            elif jdata.get(\"message\") == \"Training progress: checking checkpoint conditions\":\n",
    "                current_epochs_trained = 0 if jdata['step'] == 0 else (jdata['step'] / iterations_per_epoch)\n",
    "                saved_data[jdata[\"step\"]] = current_epochs_trained, jdata[\"train_loss\"], jdata[\"val_loss\"]\n",
    "\n",
    "    sorted_running_iters = sorted(running_data)\n",
    "    running_epochs_trained, running_train_loss, running_val_loss = zip(*(running_data[i] for i in sorted_running_iters))\n",
    "\n",
    "    saved_saved_iters = sorted(saved_data)\n",
    "    saved_epochs_trained, saved_train_loss, saved_val_loss = zip(*(saved_data[i] for i in saved_saved_iters))\n",
    "\n",
    "    # Find min loss points\n",
    "    min_val_loss_iter_idx = saved_val_loss.index(min(saved_val_loss))\n",
    "    min_saved_losses_iter = saved_saved_iters[min_val_loss_iter_idx]\n",
    "    min_saved_val_loss_epoch, min_saved_train_loss, min_saved_val_loss = saved_epochs_trained[min_val_loss_iter_idx], saved_train_loss[min_val_loss_iter_idx], saved_val_loss[min_val_loss_iter_idx]\n",
    "\n",
    "    num_iters = len(sorted_running_iters)\n",
    "    return num_iters, learning_rate, min_lr, lr_decay_iters, min_saved_train_loss, min_saved_val_loss, num_layers\n",
    "\n",
    "def plot_learning_rate_effects(model_information_dict):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    learning_rates = []\n",
    "    min_lrs = []\n",
    "    val_losses = []\n",
    "    labels = []\n",
    "    for model_name, info in model_information_dict.items():\n",
    "        learning_rates.append(info['learning_rate'])\n",
    "        min_lrs.append(info['min_lr'])\n",
    "        val_losses.append(info['min_saved_val_loss'])\n",
    "        labels.append(model_name)\n",
    "    \n",
    "     # Identify the best (lowest) original validation loss\n",
    "    raw_val_losses = [info['min_saved_val_loss'] for info in model_information_dict.values() if info['learning_rate'] > 0 and info['min_lr'] > 0 and info['min_saved_val_loss'] > 0]\n",
    "    min_idx = np.argmin(raw_val_losses)\n",
    "    \n",
    "    best_x = learning_rates[min_idx]\n",
    "    best_y = min_lrs[min_idx]\n",
    "    best_z = val_losses[min_idx]\n",
    "    best_label = labels[min_idx]\n",
    "\n",
    "    scatter = ax.scatter(learning_rates, min_lrs, val_losses, c=val_losses, cmap='viridis', s=60)\n",
    "    ax.scatter([best_x], [best_y], [best_z], color='red', s=100, label=f'Best Model: {best_label}')\n",
    "    \n",
    "    for lr, mlr, vl, label in zip(learning_rates, min_lrs, val_losses, labels):\n",
    "        ax.text(lr, mlr, vl, label, size=8)\n",
    "\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Min LR\")\n",
    "    ax.set_zlabel(\"Min Validation Loss\")\n",
    "    ax.set_title(\"Effect of Learning Rates on Min Validation Loss\")\n",
    "    fig.colorbar(scatter, ax=ax, label='Min Val Loss')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_rate_effects_log10(model_information_dict):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    learning_rates = []\n",
    "    min_lrs = []\n",
    "    val_losses = []\n",
    "    labels = []\n",
    "    for model_name, info in model_information_dict.items():\n",
    "        lr = info['learning_rate']\n",
    "        mlr = info['min_lr']\n",
    "        vl = info['min_saved_val_loss']\n",
    "\n",
    "        # Skip invalid or non-positive values\n",
    "        if lr > 0 and mlr > 0 and vl > 0:\n",
    "            learning_rates.append(np.log10(lr))\n",
    "            min_lrs.append(np.log10(mlr))\n",
    "            val_losses.append(np.log10(vl))\n",
    "            labels.append(model_name)\n",
    "\n",
    "    # Identify the best (lowest) original validation loss\n",
    "    raw_val_losses = [info['min_saved_val_loss'] for info in model_information_dict.values() if info['learning_rate'] > 0 and info['min_lr'] > 0 and info['min_saved_val_loss'] > 0]\n",
    "    min_idx = np.argmin(raw_val_losses)\n",
    "    \n",
    "    best_x = learning_rates[min_idx]\n",
    "    best_y = min_lrs[min_idx]\n",
    "    best_z = val_losses[min_idx]\n",
    "    best_label = labels[min_idx]\n",
    "\n",
    "    scatter = ax.scatter(learning_rates, min_lrs, val_losses, c=val_losses, cmap='viridis', s=60)\n",
    "\n",
    "    # Highlight the best point\n",
    "    ax.scatter([best_x], [best_y], [best_z], color='red', s=100, label=f'Best Model: {best_label}')\n",
    "\n",
    "    for x, y, z, label in zip(learning_rates, min_lrs, val_losses, labels):\n",
    "        ax.text(x, y, z, label, size=8)\n",
    "\n",
    "    # Axis labels in log10 terms\n",
    "    ax.set_xlabel(\"log10(Learning Rate)\")\n",
    "    ax.set_ylabel(\"log10(Min LR)\")\n",
    "    ax.set_zlabel(\"log10(Min Validation Loss)\")\n",
    "    ax.set_title(\"Effect of Learning Rates on Min Validation Loss\")\n",
    "    fig.colorbar(scatter, ax=ax, label='log10(Min Val Loss)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_model_info_table(model_information_dict):\n",
    "    # Sort model data by min_saved_val_loss\n",
    "    sorted_items = sorted(\n",
    "        model_information_dict.items(),\n",
    "        key=lambda x: x[1]['min_saved_val_loss']\n",
    "    )\n",
    "\n",
    "    # Prepare table data\n",
    "    columns = [\"Model Name\", \"Num Iters\", \"Learning Rate\", \"Min LR\", \"Decay Iters\", \"Avg Learning Rate\", \"Num Layers\", \"Min Val Loss\"]\n",
    "    cell_data = [\n",
    "        [\n",
    "            model_name,\n",
    "            f\"{info['num_iters']}\",\n",
    "            f\"{info['learning_rate']:.2e}\",\n",
    "            f\"{info['min_lr']:.2e}\",\n",
    "            f\"{info['lr_decay_iters']:.5f}\",\n",
    "            f\"{(info['learning_rate'] - info['min_lr']) / info['lr_decay_iters']:.2e}\",\n",
    "            f\"{info['num_layers']}\",\n",
    "            f\"{info['min_saved_val_loss']:.5f}\"\n",
    "        ]\n",
    "        for model_name, info in sorted_items\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, len(cell_data) * 0.4 + 1))\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=cell_data,\n",
    "        colLabels=columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)\n",
    "    plt.title(\"Model Training Summary (Sorted by Min Val Loss)\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "models_to_compare = ['model_5_3_1', 'model_5_3_2', 'model_5_3_3', 'model_5_3_4', 'model_5_3_8', \n",
    "                     'model_5_3_9', 'model_5_3_10', 'model_5_3_11', 'model_5_3_12', 'model_5_3_13', \n",
    "                     'model_5_3_14', 'model_5_3_15', 'model_5_3_16', 'model_5_3_17', 'model_5_3_18']\n",
    "model_information_dict = {}\n",
    "for model_name in models_to_compare:\n",
    "    num_iters, learning_rate, min_lr, lr_decay_iters, min_saved_train_loss, min_saved_val_loss, num_layers = get_train_data_for_model(model_name)\n",
    "    model_information_dict[model_name] = {\n",
    "        \"num_iters\": num_iters,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"min_lr\": min_lr,\n",
    "        \"lr_decay_iters\": lr_decay_iters,\n",
    "        \"min_saved_train_loss\": min_saved_train_loss,\n",
    "        \"min_saved_val_loss\": min_saved_val_loss,\n",
    "        \"num_layers\": num_layers\n",
    "    }\n",
    "\n",
    "summary_stats = {}\n",
    "value_keys = next(iter(model_information_dict.values())).keys()\n",
    "for key in value_keys:\n",
    "    all_values = [info[key] for info in model_information_dict.values()]\n",
    "    summary_stats[key] = {\n",
    "        'min': min(all_values),\n",
    "        'max': max(all_values)\n",
    "    }\n",
    "\n",
    "for key, stats in summary_stats.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Min: {stats['min']}\")\n",
    "    print(f\"  Max: {stats['max']}\")\n",
    "\n",
    "# models_to_compare = ['model_5_3_1', 'model_5_3_2', 'model_5_3_3', 'model_5_3_4', 'model_5_3_5', 'model_5_3_6', 'model_5_3_7', 'model_5_3_8', 'model_5_3_9', 'model_5_3_10', 'model_5_3_11', 'model_5_3_12',]\n",
    "# plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)\n",
    "# models_to_compare = ['model_5_3_9', 'model_5_3_10', 'model_5_3_12']\n",
    "# plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, y_lim=[0.78, 0.85], use_log_scale=True)\n",
    "\n",
    "print(json.dumps(model_information_dict, indent=4))\n",
    "create_model_info_table(model_information_dict)\n",
    "plot_learning_rate_effects(model_information_dict)\n",
    "plot_learning_rate_effects_log10(model_information_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Investigating the broken eta distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "columns = [\"num_particles\", \"pdgid\", \"e\", \"px\", \"py\", \"pz\", \"eta\", \"theta\", \"phi\"]\n",
    "\n",
    "def get_common_data(model_name):\n",
    "    dictionary_filename = pUtil.get_model_preparation_dir(model_name) / 'dictionary.json'\n",
    "    # real_leading_test_particles_filename = pUtil.get_model_preparation_dir(model_name) / 'real_leading_test_particles.csv'\n",
    "    real_leading_test_particles_filename = pUtil.get_temp_dir() / 'real_leading_test_particles.txt'\n",
    "    sampled_leading_particles_filename = pUtil.get_latest_sampling_dir(model_name) / 'sampled_leading_particles.csv'\n",
    "\n",
    "    with open(dictionary_filename) as dictionary_file:\n",
    "        dictionary = json.load(dictionary_file)\n",
    "\n",
    "    # Convenience dictionary definitions\n",
    "    p_bin_count = (dictionary[\"e_bin_data\"][\"max\"] - dictionary[\"e_bin_data\"][\"min\"]) // 1000\n",
    "    e_bin_count = (dictionary[\"e_bin_data\"][\"max\"] - dictionary[\"e_bin_data\"][\"min\"]) // dictionary[\"e_bin_data\"][\"step_size\"]\n",
    "    eta_bin_count = int((dictionary[\"eta_bin_data\"][\"max\"] - dictionary[\"eta_bin_data\"][\"min\"]) // dictionary[\"eta_bin_data\"][\"step_size\"])\n",
    "            \n",
    "    bin_settings = {\n",
    "        \"num_particles\": { \"min\": 0,                                 \"max\": 50,                                \"bins\": 50 },\n",
    "        \"e\":             { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": e_bin_count },\n",
    "        \"px\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"py\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"pz\":            { \"min\": dictionary[\"e_bin_data\"][\"min\"],   \"max\": dictionary[\"e_bin_data\"][\"max\"],   \"bins\": p_bin_count },\n",
    "        \"eta\":           { \"min\": dictionary[\"eta_bin_data\"][\"min\"], \"max\": dictionary[\"eta_bin_data\"][\"max\"], \"bins\": eta_bin_count },\n",
    "        \"theta\":         { \"min\": -2 * np.pi,                        \"max\": 2 * np.pi,                         \"bins\": int((4 * np.pi) // dictionary[\"theta_bin_data\"][\"step_size\"]) },\n",
    "        \"phi\":           { \"min\": -2 * np.pi,                        \"max\": 2 * np.pi,                         \"bins\": int((4 * np.pi) // dictionary[\"phi_bin_data\"][\"step_size\"]) },\n",
    "    }\n",
    "\n",
    "    df1 = pd.read_csv(real_leading_test_particles_filename, sep=\" \", names=columns, engine=\"c\", header=None)\n",
    "    df2 = pd.read_csv(sampled_leading_particles_filename, sep=\" \", names=columns, engine=\"c\", header=None)\n",
    "    return bin_settings, df1, df2\n",
    "\n",
    "def generate_distributions(model_name, column_name, ax=None):\n",
    "    bin_settings, df1, df2 = get_common_data(model_name)\n",
    "    \n",
    "    min_val = bin_settings[column_name]['min']\n",
    "    max_val = bin_settings[column_name]['max']\n",
    "    bins = bin_settings[column_name]['bins']\n",
    "    \n",
    "    df1_weights = np.ones_like(df1[column_name]) / len(df1[column_name])\n",
    "    df2_weights = np.ones_like(df2[column_name]) / len(df2[column_name])\n",
    "    \n",
    "    ax = ax or plt\n",
    "    ax.hist(df1[column_name], bins=bins, weights=df1_weights, range=(min_val, max_val), edgecolor=\"black\", alpha=0.7, color=\"blue\", label=f'Input ({model_name})')\n",
    "    ax.hist(df2[column_name], bins=bins, weights=df2_weights, range=(min_val, max_val), edgecolor=\"black\", alpha=0.7, color=\"orange\", label=f'Sampled ({model_name})')\n",
    "    if ax is not plt:\n",
    "        ax.set_xlabel(column_name)\n",
    "        ax.set_ylabel('Frequency (Normalized)')\n",
    "        ax.set_title(f'{model_name}')\n",
    "        ax.legend()\n",
    "\n",
    "def compare_distributions(models_to_compare, column_name, juxtaposed=True, dists_per_row=3):\n",
    "    if juxtaposed:\n",
    "        num_horizontal, num_vertical = min(len(models_to_compare), dists_per_row), (math.ceil(len(models_to_compare) / dists_per_row))\n",
    "        figure, axes = plt.subplots(num_vertical, num_horizontal, figsize=(8 * num_horizontal, 6 * num_vertical), sharex=False, sharey=True)\n",
    "        if len(models_to_compare) == 1:\n",
    "            axes = [axes]\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "        for model_name, ax in zip(models_to_compare, axes):\n",
    "            generate_distributions(model_name, column_name=column_name, ax=ax)\n",
    "        figure.suptitle(f'Training Progress for {\", \".join(models_to_compare)}')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        for model_name in models_to_compare:\n",
    "            generate_distributions(model_name, column_name=column_name)\n",
    "        plt.title(f'Training Progress for {\", \".join(models_to_compare)}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "models_to_compare = ['model_5_2_1']\n",
    "for column in columns:\n",
    "    if column == 'pdgid':\n",
    "        continue\n",
    "    compare_distributions(models_to_compare, column_name=column, juxtaposed=True, dists_per_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eta distribution seems to be fine. There is something about the tokenizer that creates the brokenness in the distributions.\n",
    "Furthermore this shows we should probably start comparing tokenized to tokenized not real to tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#db7d60\">Keeping track of things</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_default_df(['model_5_6_1', 'model_5_6_2', 'model_5_6_3', 'model_5_6_4'])\n",
    "df.plot(kind='scatter', x = 'learning_rate', y = 'min_saved_val_loss')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a mega table full of everything from all the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = pUtil.get_all_model_names()\n",
    "df = get_default_df(all_model_names)\n",
    "df = df.applymap(lambda x: f\"{x:,}\" if isinstance(x, int) else x)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = pUtil.get_all_model_names()\n",
    "plot_train_graphs(all_model_names, juxtaposed=False, use_epochs=False, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a train graph for all the models trained on 5_3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    'model_5_3_1',\n",
    "    'model_5_3_2',\n",
    "    'model_5_3_3',\n",
    "    'model_5_3_4',\n",
    "    'model_5_3_5',\n",
    "    'model_5_3_6',\n",
    "    'model_5_3_7',\n",
    "    'model_5_3_8',\n",
    "    'model_5_3_9',\n",
    "    'model_5_3_10',\n",
    "    'model_5_3_11',\n",
    "    'model_5_3_12',\n",
    "    'model_5_3_13',\n",
    "    'model_5_3_14',\n",
    "    'model_5_3_15',\n",
    "    'model_5_3_16',\n",
    "    'model_5_3_17',\n",
    "    'model_5_3_18',\n",
    "    'model_5_3_19',\n",
    "    'model_5_3_20',\n",
    "    'model_5_3_21'\n",
    "]\n",
    "\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True, y_lim=[0.7, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking models without particle boundaries. All of the 5_5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    'model_5_5_1',\n",
    "    'model_5_5_2',\n",
    "    'model_5_5_3',\n",
    "    'model_5_5_4',\n",
    "    'model_5_5_5',\n",
    "    'model_5_5_6'\n",
    "]\n",
    "\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True, y_lim=[1, 2])\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True, y_lim=[1, 1.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking models without particles boundaries in the padding (paddingv2 models). All of the 5_6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    'model_5_6_1',\n",
    "    'model_5_6_2',\n",
    "    'model_5_6_3',\n",
    "    'model_5_6_4'\n",
    "]\n",
    "\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the different scheme based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = pUtil.get_all_model_names()\n",
    "df = get_default_df(all_model_names)\n",
    "df = df.applymap(lambda x: f\"{x:,}\" if isinstance(x, int) else x)\n",
    "\n",
    "df = df[df['preparation_name'].isin(['preparation_5_4', 'preparation_5_5', 'preparation_5_6'])]\n",
    "df_sorted_by_min_val_loss = df.sort_values(by=\"min_saved_val_loss\", ascending=True)\n",
    "\n",
    "# df_sorted_by_min_val_loss = df_sorted_by_min_val_loss.style.background_gradient(cmap='RdYlGn_r', subset=['min_saved_val_loss'])\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "display(df_sorted_by_min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = pUtil.get_all_model_names()\n",
    "df = get_default_df(all_model_names)\n",
    "df = df[df['preparation_name'].isin(['preparation_5_4', 'preparation_5_5', 'preparation_5_6'])]\n",
    "model_names_list = df['model_name'].tolist()\n",
    "plot_train_graphs(model_names_list, juxtaposed=False, use_epochs=False, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is to check the relationship between val loss and the distributions.\n",
    "\n",
    "Basically, even though the val loss is high after some changes* I think the distributions will be fine. This portion will be used to compare both the distributions visually and also the metrics.\n",
    "* Changes meaning the type embedding and ignoring the padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = ['model_5_3_22', 'model_5_5_4', 'model_5_5_5', 'model_5_5_6', 'model_5_6_1', 'model_5_6_2', 'model_5_6_3', 'model_5_6_4']\n",
    "df = get_default_df(models_to_compare)\n",
    "df = df.applymap(lambda x: f\"{x:,}\" if isinstance(x, int) else x)\n",
    "\n",
    "df_sorted_by_min_val_loss = df.sort_values(by=\"min_saved_val_loss\", ascending=True)\n",
    "display(df_sorted_by_min_val_loss)\n",
    "\n",
    "plot_train_graphs(models_to_compare, juxtaposed=False, use_epochs=False, use_log_scale=True)\n",
    "\n",
    "models_to_compare = ['model_5_3_22', 'model_5_5_6', 'model_5_6_2', 'model_5_6_4']\n",
    "for column in columns:\n",
    "    if column == 'pdgid':\n",
    "        continue\n",
    "    compare_distributions(models_to_compare, column_name=column, juxtaposed=True, dists_per_row=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
