W0628 22:47:43.806000 140283831670592 torch/distributed/run.py:779] 
W0628 22:47:43.806000 140283831670592 torch/distributed/run.py:779] *****************************************
W0628 22:47:43.806000 140283831670592 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0628 22:47:43.806000 140283831670592 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
Configurator found file config/model_5_9_5.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Initializing a new model from scratch
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Training progress: saving current checkpoint @ val_loss 9.787761688232422
AllOutput: Training progress: saving best checkpoint @ val_loss 4.954986095428467
AllOutput: Training progress: saving current checkpoint @ val_loss 4.954986095428467
AllOutput: Training progress: saving best checkpoint @ val_loss 4.751842498779297
AllOutput: Training progress: saving current checkpoint @ val_loss 4.751842498779297
AllOutput: Training progress: saving best checkpoint @ val_loss 4.696110248565674
AllOutput: Training progress: saving current checkpoint @ val_loss 4.696110248565674
AllOutput: Training progress: saving best checkpoint @ val_loss 4.651658535003662
AllOutput: Training progress: saving current checkpoint @ val_loss 4.651658535003662
AllOutput: Training progress: saving best checkpoint @ val_loss 4.63997745513916
AllOutput: Training progress: saving current checkpoint @ val_loss 4.63997745513916
AllOutput: Training progress: saving best checkpoint @ val_loss 4.6240644454956055
AllOutput: Training progress: saving current checkpoint @ val_loss 4.6240644454956055
AllOutput: Training progress: saving best checkpoint @ val_loss 4.600549697875977
AllOutput: Training progress: saving current checkpoint @ val_loss 4.600549697875977
AllOutput: Training progress: saving best checkpoint @ val_loss 4.571138858795166
AllOutput: Training progress: saving current checkpoint @ val_loss 4.571138858795166
AllOutput: Training progress: saving best checkpoint @ val_loss 4.55436897277832
AllOutput: Training progress: saving current checkpoint @ val_loss 4.55436897277832
AllOutput: Training progress: saving best checkpoint @ val_loss 4.543358325958252
AllOutput: Training progress: saving current checkpoint @ val_loss 4.543358325958252
AllOutput: Training progress: saving best checkpoint @ val_loss 4.534022331237793
AllOutput: Training progress: saving current checkpoint @ val_loss 4.534022331237793
AllOutput: Training progress: saving best checkpoint @ val_loss 4.528347969055176
AllOutput: Training progress: saving current checkpoint @ val_loss 4.528347969055176
AllOutput: Training progress: saving best checkpoint @ val_loss 4.508010387420654
AllOutput: Training progress: saving current checkpoint @ val_loss 4.508010387420654
AllOutput: Training progress: saving best checkpoint @ val_loss 4.500350475311279
AllOutput: Training progress: saving current checkpoint @ val_loss 4.500350475311279
AllOutput: Training progress: saving best checkpoint @ val_loss 4.496377944946289
AllOutput: Training progress: saving current checkpoint @ val_loss 4.496377944946289
AllOutput: Training progress: saving best checkpoint @ val_loss 4.494643211364746
AllOutput: Training progress: saving current checkpoint @ val_loss 4.494643211364746
AllOutput: Training progress: saving best checkpoint @ val_loss 4.490464687347412
AllOutput: Training progress: saving current checkpoint @ val_loss 4.490464687347412
AllOutput: Training progress: saving best checkpoint @ val_loss 4.486812114715576
AllOutput: Training progress: saving current checkpoint @ val_loss 4.486812114715576
AllOutput: Training progress: saving best checkpoint @ val_loss 4.484568119049072
AllOutput: Training progress: saving current checkpoint @ val_loss 4.484568119049072
AllOutput: Training progress: saving best checkpoint @ val_loss 4.483884334564209
AllOutput: Training progress: saving current checkpoint @ val_loss 4.483884334564209
AllOutput: Training progress: saving best checkpoint @ val_loss 4.483522415161133
AllOutput: Training progress: saving current checkpoint @ val_loss 4.483522415161133
AllOutput: Training progress: saving best checkpoint @ val_loss 4.482739448547363
AllOutput: Training progress: saving current checkpoint @ val_loss 4.482739448547363
AllOutput: Training progress: saving best checkpoint @ val_loss 4.482122898101807
AllOutput: Training progress: saving current checkpoint @ val_loss 4.482122898101807
AllOutput: Training progress: saving best checkpoint @ val_loss 4.48185396194458
AllOutput: Training progress: saving current checkpoint @ val_loss 4.48185396194458
AllOutput: Training progress: saving best checkpoint @ val_loss 4.480432033538818
AllOutput: Training progress: saving current checkpoint @ val_loss 4.480432033538818
AllOutput: Training progress: saving best checkpoint @ val_loss 4.47941780090332
AllOutput: Training progress: saving current checkpoint @ val_loss 4.47941780090332
AllOutput: Training progress: saving current checkpoint @ val_loss 4.479571342468262
slurmstepd: error: *** STEP 39995051.0 ON nid008596 CANCELLED AT 2025-07-01T05:47:39 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 39995051 ON nid008596 CANCELLED AT 2025-07-01T05:47:39 DUE TO TIME LIMIT ***
