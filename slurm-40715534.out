W0718 09:10:31.302000 139817253959488 torch/distributed/run.py:779] 
W0718 09:10:31.302000 139817253959488 torch/distributed/run.py:779] *****************************************
W0718 09:10:31.302000 139817253959488 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0718 09:10:31.302000 139817253959488 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
Configurator found file config/model_5_10_2.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_10_2
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_10_2
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_10_2
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_5_10_2
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Iterations per epoch is 863
AllOutput: Training progress: saving current checkpoint @ val_loss 6.0579447746276855
AllOutput: Training progress: saving best checkpoint @ val_loss 6.034313201904297
AllOutput: Training progress: saving current checkpoint @ val_loss 6.034313201904297
AllOutput: Training progress: saving best checkpoint @ val_loss 6.033891201019287
AllOutput: Training progress: saving current checkpoint @ val_loss 6.033891201019287
AllOutput: Training progress: saving best checkpoint @ val_loss 6.0331902503967285
AllOutput: Training progress: saving current checkpoint @ val_loss 6.0331902503967285
AllOutput: Training progress: saving current checkpoint @ val_loss 6.036015510559082
AllOutput: Training progress: saving current checkpoint @ val_loss 6.035788536071777
AllOutput: Training progress: saving current checkpoint @ val_loss 6.035099506378174
AllOutput: Training progress: saving current checkpoint @ val_loss 6.034412384033203
AllOutput: Training progress: saving best checkpoint @ val_loss 6.028968334197998
AllOutput: Training progress: saving current checkpoint @ val_loss 6.028968334197998
AllOutput: Training progress: saving best checkpoint @ val_loss 6.028691291809082
AllOutput: Training progress: saving current checkpoint @ val_loss 6.028691291809082
AllOutput: Training progress: saving best checkpoint @ val_loss 6.028143882751465
AllOutput: Training progress: saving current checkpoint @ val_loss 6.028143882751465
AllOutput: Training progress: saving current checkpoint @ val_loss 6.031639575958252
AllOutput: Training progress: saving current checkpoint @ val_loss 6.031660556793213
AllOutput: Training progress: saving current checkpoint @ val_loss 6.031361103057861
AllOutput: Training progress: saving current checkpoint @ val_loss 6.031293869018555
AllOutput: Training progress: saving current checkpoint @ val_loss 6.030333042144775
AllOutput: Training progress: saving current checkpoint @ val_loss 6.030064105987549
AllOutput: Training progress: saving current checkpoint @ val_loss 6.029727935791016
AllOutput: Training progress: saving best checkpoint @ val_loss 6.027917385101318
AllOutput: Training progress: saving current checkpoint @ val_loss 6.027917385101318
AllOutput: Training progress: saving current checkpoint @ val_loss 6.028380870819092
AllOutput: Training progress: saving best checkpoint @ val_loss 6.027310848236084
AllOutput: Training progress: saving current checkpoint @ val_loss 6.027310848236084
AllOutput: Training progress: saving best checkpoint @ val_loss 6.025763988494873
AllOutput: Training progress: saving current checkpoint @ val_loss 6.025763988494873
AllOutput: Training progress: saving best checkpoint @ val_loss 6.0257463455200195
AllOutput: Training progress: saving current checkpoint @ val_loss 6.0257463455200195
AllOutput: Training progress: saving best checkpoint @ val_loss 6.025314807891846
AllOutput: Training progress: saving current checkpoint @ val_loss 6.025314807891846
AllOutput: Training progress: saving current checkpoint @ val_loss 6.025383472442627
AllOutput: Training progress: saving current checkpoint @ val_loss 6.025911808013916
slurmstepd: error: *** STEP 40715534.0 ON nid008617 CANCELLED AT 2025-07-20T16:10:19 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40715534 ON nid008617 CANCELLED AT 2025-07-20T16:10:19 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
