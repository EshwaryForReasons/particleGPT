Sun Jul 13 05:39:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:03:00.0 Off |                    0 |
| N/A   29C    P0             60W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   27C    P0             62W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:82:00.0 Off |                    0 |
| N/A   28C    P0             63W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0             63W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
W0713 05:39:18.423000 140538342999872 torch/distributed/run.py:779] 
W0713 05:39:18.423000 140538342999872 torch/distributed/run.py:779] *****************************************
W0713 05:39:18.423000 140538342999872 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0713 05:39:18.423000 140538342999872 torch/distributed/run.py:779] *****************************************
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
Configurator found file config/model_6_1_1.json.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Training started.
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Resuming training from /pscratch/sd/e/eshy/particleGPT/trained_models/model_6_1_1
/pscratch/sd/e/eshy/particleGPT/train.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(running_ckpt_path, map_location=conf.training.device)
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Iterations per epoch is 8711
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4911580085754395
AllOutput: Training progress: saving current checkpoint @ val_loss 4.475396633148193
AllOutput: Training progress: saving best checkpoint @ val_loss 4.474865436553955
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474865436553955
AllOutput: Training progress: saving best checkpoint @ val_loss 4.474313259124756
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474313259124756
AllOutput: Training progress: saving current checkpoint @ val_loss 4.474526882171631
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473923206329346
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473923206329346
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473833084106445
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473833084106445
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473660945892334
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473660945892334
AllOutput: Training progress: saving best checkpoint @ val_loss 4.473176002502441
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473176002502441
AllOutput: Training progress: saving current checkpoint @ val_loss 4.473223686218262
AllOutput: Training progress: saving best checkpoint @ val_loss 4.472165584564209
AllOutput: Training progress: saving current checkpoint @ val_loss 4.472165584564209
AllOutput: Training progress: saving current checkpoint @ val_loss 4.472387313842773
AllOutput: Training progress: saving current checkpoint @ val_loss 4.472443580627441
AllOutput: Training progress: saving current checkpoint @ val_loss 4.472362518310547
AllOutput: Training progress: saving best checkpoint @ val_loss 4.471792221069336
AllOutput: Training progress: saving current checkpoint @ val_loss 4.471792221069336
AllOutput: Training progress: saving best checkpoint @ val_loss 4.471438884735107
AllOutput: Training progress: saving current checkpoint @ val_loss 4.471438884735107
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4711689949035645
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4711689949035645
AllOutput: Training progress: saving best checkpoint @ val_loss 4.47086238861084
AllOutput: Training progress: saving current checkpoint @ val_loss 4.47086238861084
AllOutput: Training progress: saving current checkpoint @ val_loss 4.471125602722168
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4707746505737305
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4707746505737305
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4709882736206055
AllOutput: Training progress: saving best checkpoint @ val_loss 4.470442295074463
AllOutput: Training progress: saving current checkpoint @ val_loss 4.470442295074463
AllOutput: Training progress: saving best checkpoint @ val_loss 4.470417499542236
AllOutput: Training progress: saving current checkpoint @ val_loss 4.470417499542236
AllOutput: Training progress: saving best checkpoint @ val_loss 4.470240592956543
AllOutput: Training progress: saving current checkpoint @ val_loss 4.470240592956543
AllOutput: Training progress: saving best checkpoint @ val_loss 4.469831466674805
AllOutput: Training progress: saving current checkpoint @ val_loss 4.469831466674805
AllOutput: Training progress: saving best checkpoint @ val_loss 4.4695940017700195
AllOutput: Training progress: saving current checkpoint @ val_loss 4.4695940017700195
slurmstepd: error: *** STEP 40540091.0 ON nid008592 CANCELLED AT 2025-07-15T12:39:14 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 40540091 ON nid008592 CANCELLED AT 2025-07-15T12:39:14 DUE TO TIME LIMIT ***
